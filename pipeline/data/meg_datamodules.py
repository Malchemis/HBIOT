"""Lightning DataModules for MEG spike detection.

This module provides DataModule implementations for training, validation, testing,
and prediction on MEG data. Supports both preloaded and on-the-fly data processing.
"""
import logging
import pickle
import os
import random
from typing import Dict, List, Tuple, Optional, Union, Any

import lightning as L
import numpy as np
import torch
from torch.utils.data._utils.collate import default_collate

from pipeline.data.meg_datasets import PreloadedDataset, PredictDataset, OnlineWindowDataset


logger: logging.Logger = logging.getLogger(__name__)


class MEGOnTheFlyDataModule(L.LightningDataModule):
    """Lightning DataModule for on-the-fly MEG spike detection data.

    This DataModule loads raw MEG files and processes them on-demand using split files
    generated by generate_splits.py. Supports cross-validation by loading different fold
    files and is compatible with distributed training (DDP).

    Attributes:
        splits_dir: Directory containing split files from generate_splits.py.
        dataset_config: Configuration dictionary for data processing.
        fold: Current fold number for cross-validation.
        batch_size: Batch size for dataloaders.
        num_workers: Number of workers for dataloaders.
        seed: Random seed for reproducibility.
    """

    def __init__(
        self,
        splits_dir: str,
        dataloader_config: Dict[str, Any],
        dataset_config: Dict[str, Any],
        fold: int = 1,
        seed: int = 42,
        prepare_data_per_node: bool = False,
        root_dirs: Optional[List[str]] = None,
        auto_generate_splits: bool = False,
        n_folds: int = 5,
        reference_coordinates: str = "",
        dataset_name: str = "PreloadedDataset",
        preprocessed_dir: Optional[str] = None,
        samples_per_recording: int = 10,
        force_preprocess: bool = False,
        **kwargs: Any
    ):
        """Initialize the on-the-fly MEG DataModule.

        Args:
            splits_dir: Directory containing split files from generate_splits.py.
            dataloader_config: Configuration dictionary for dataloaders.
            dataset_config: Configuration dictionary for data processing.
            fold: Fold number for cross-validation (1-based).
            seed: Random seed for reproducibility.
            prepare_data_per_node: Whether to prepare data per node in DDP.
            root_dirs: Root directories to search for data files.
            auto_generate_splits: Whether to automatically generate splits if not found.
            n_folds: Number of folds for cross-validation.
            reference_coordinates: Path to the reference file for channel names.
            dataset_name: Name of dataset class ('PreloadedDataset' or 'OnlineWindowDataset').
            preprocessed_dir: Directory for preprocessed files (OnlineWindowDataset only).
            samples_per_recording: Chunks per recording per epoch (OnlineWindowDataset only).
            force_preprocess: Force reprocessing of cached files (OnlineWindowDataset only).
            **kwargs: Additional parameters for future use or compatibility.
        """
        super().__init__()
        self.splits_dir = splits_dir
        self.dataloader_config = dataloader_config
        self.dataset_config = dataset_config
        self.fold = fold
        self.seed = seed
        self.root_dirs = root_dirs or []
        self.auto_generate_splits = auto_generate_splits
        self.n_folds = n_folds

        self.dataset_name = dataset_name
        self.preprocessed_dir = preprocessed_dir
        self.samples_per_recording = samples_per_recording
        self.force_preprocess = force_preprocess

        self.dataset_registry = {
            'PreloadedDataset': PreloadedDataset,
            'OnlineWindowDataset': OnlineWindowDataset,
        }

        if self.dataset_name not in self.dataset_registry:
            raise ValueError(f"Unknown dataset name: {self.dataset_name}. "
                           f"Available: {list(self.dataset_registry.keys())}")

        self.dataset_class = self.dataset_registry[self.dataset_name]

        self.batch_size = dataloader_config.get('train', {}).get('batch_size', 32)
        self.num_workers = dataloader_config.get('train', {}).get('num_workers', 4)

        with open(reference_coordinates, 'rb') as f:
            self.good_channels = pickle.load(f)

        self.prepare_data_per_node = prepare_data_per_node

        self.train_dataset: Optional[PreloadedDataset | OnlineWindowDataset] = None
        self.val_dataset: Optional[PreloadedDataset | OnlineWindowDataset] = None
        self.test_dataset: Optional[PreloadedDataset | OnlineWindowDataset] = None

        self.input_shape: Optional[torch.Size] = None
        self.output_shape: Optional[torch.Size] = None

        self.logger = logging.getLogger(__name__)

        self._set_random_seeds()

    def _set_random_seeds(self) -> None:
        """Set random seeds for reproducibility."""
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        torch.cuda.manual_seed(self.seed)
        torch.cuda.manual_seed_all(self.seed)
        # torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.use_deterministic_algorithms(True, warn_only=True)

    @staticmethod
    def seed_worker(worker_id: int) -> None:
        """Set seed for data loader workers to ensure reproducibility.

        Args:
            worker_id: ID of the worker process.
        """
        worker_seed = torch.initial_seed() % 2 ** 32
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    def prepare_data(self) -> None:
        """Check that split files exist or generate them if needed.

        Raises:
            FileNotFoundError: If split files don't exist and auto_generate_splits is False.
        """
        fold_file = os.path.join(self.splits_dir, f"fold_{self.fold}.json")
        test_file = os.path.join(self.splits_dir, "test_files.json")
        splits_exist = (
            os.path.exists(self.splits_dir) and
            os.path.exists(fold_file) and
            os.path.exists(test_file)
        )

        if not splits_exist and self.auto_generate_splits:
            self.logger.info(f"Split files not found in {self.splits_dir}. Generating splits automatically...")
            self._generate_splits()
        elif not splits_exist:
            raise FileNotFoundError(
                f"Split files not found in {self.splits_dir}. "
                f"Set auto_generate_splits=True to generate them automatically, "
                f"or run: python scripts/generate_splits.py --config configs/config-splits.yaml"
            )

        if not os.path.exists(self.splits_dir):
            raise FileNotFoundError(f"Splits directory not found: {self.splits_dir}")
        if not os.path.exists(fold_file):
            raise FileNotFoundError(f"Fold file not found: {fold_file}")
        if not os.path.exists(test_file):
            raise FileNotFoundError(f"Test file not found: {test_file}")

        self.logger.info(f"Using splits from {self.splits_dir}")

    def _generate_splits(self) -> None:
        """Generate cross-validation splits using the generate_splits module.

        Automatically called if splits don't exist and auto_generate_splits=True.

        Raises:
            ValueError: If root_dirs is not specified.
            RuntimeError: If split generation fails.
        """
        try:
            import sys
            from pathlib import Path

            scripts_dir = Path(__file__).parent.parent.parent / "scripts"
            sys.path.insert(0, str(scripts_dir))

            from scripts.generate_splits import generate_splits
            from pipeline.data.preprocessing.annotation import compile_annotation_patterns
            split_config = {
                'root_dirs': self.root_dirs,
                'n_splits': self.n_folds,
                'random_state': self.seed,
                'splits_output_dir': self.splits_dir,
                'annotation_rules': self.dataset_config.get('annotation_rules', {}),
                'skip_files': self.dataset_config.get('skip_files', []),
            }

            if not self.root_dirs:
                raise ValueError("root_dirs must be specified for automatic split generation")

            self.logger.info(f"Generating {split_config['n_splits']} folds...")
            self.logger.info(f"Output directory: {self.splits_dir}")

            generate_splits(split_config)

            self.logger.info("Split generation completed successfully")

        except Exception as e:
            self.logger.error(f"Failed to generate splits: {e}")
            raise RuntimeError(
                f"Automatic split generation failed: {e}\n"
                f"Please run manually: python scripts/generate_splits.py --config configs/config-splits.yaml"
            )

    def setup(self, stage: Optional[str] = None) -> None:
        """Set up the datasets based on stage.

        Args:
            stage: Current stage ('fit', 'validate', 'test', or None).
        """
        fold_file = os.path.join(self.splits_dir, f"fold_{self.fold}.json")
        test_file = os.path.join(self.splits_dir, "test_files.json")

        dataset_kwargs = self._get_dataset_kwargs()
        if stage == 'fit' or stage is None:
            if self.dataset_name == 'PreloadedDataset':
                if self.train_dataset is None or not getattr(self.train_dataset, 'preloaded', False):
                    self.train_dataset = self.dataset_class.from_split_file(
                        fold_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        split_type='train',
                        **dataset_kwargs
                    )

                if self.val_dataset is None or not getattr(self.val_dataset, 'preloaded', False):
                    self.val_dataset = self.dataset_class.from_split_file(
                        fold_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        split_type='val',
                        **dataset_kwargs
                    )
                self.preload_data(stage='fit')
            else:
                if self.train_dataset is None:
                    self.train_dataset = self.dataset_class.from_split_file(
                        fold_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        split_type='train',
                        **dataset_kwargs
                    )

                if self.val_dataset is None:
                    self.val_dataset = self.dataset_class.from_split_file(
                        fold_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        split_type='val',
                        **dataset_kwargs
                    )

        if stage == 'test':
            if self.dataset_name == 'PreloadedDataset':
                if self.test_dataset is None or not getattr(self.test_dataset, 'preloaded', False):
                    self.test_dataset = self.dataset_class.from_test_file(
                        test_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        **dataset_kwargs
                    )
                self.preload_data(stage='test')
            else:
                if self.test_dataset is None:
                    self.test_dataset = self.dataset_class.from_test_file(
                        test_file,
                        self.dataset_config,
                        good_channels=self.good_channels,
                        **dataset_kwargs
                    )

    def _get_dataset_kwargs(self) -> Dict[str, Any]:
        """Get dataset-specific keyword arguments.

        Returns:
            Dictionary of kwargs for the selected dataset class.
        """
        if self.dataset_name == 'PreloadedDataset':
            return {
                'n_workers': self.num_workers,
                'is_test': False,
            }
        elif self.dataset_name == 'OnlineWindowDataset':
            if self.preprocessed_dir is None:
                raise ValueError("preprocessed_dir must be set for OnlineWindowDataset")
            return {
                'preprocessed_dir': self.preprocessed_dir,
                'samples_per_recording': self.samples_per_recording,
                'force_preprocess': self.force_preprocess,
            }
        else:
            return {}

    def preload_data(self, stage: Optional[str] = None) -> None:
        """Preload data for training and validation into memory for faster access.

        Args:
            stage: Current stage ('fit', 'validate', 'test', or None).
        """
        self.logger.info("Explicitly preloading datasets...")
        
        if self.train_dataset is None and self.val_dataset is None and self.test_dataset is None:
            self.logger.warning("No datasets to preload")
            return
        
        if isinstance(self.train_dataset, OnlineWindowDataset) or \
            isinstance(self.val_dataset, OnlineWindowDataset) or \
                isinstance(self.test_dataset, OnlineWindowDataset):
            self.logger.info("Dataset is OnlineWindowDataset - data is loaded on-the-fly, skipping preload")
            return
        
        if stage is None or stage == 'fit':
            if self.train_dataset is not None and not self.train_dataset.preloaded:
                self.logger.info("Preloading training dataset...")
                self.train_dataset.preload()
                
            if self.val_dataset is not None and not self.val_dataset.preloaded:
                self.logger.info("Preloading validation dataset...")
                self.val_dataset.preload()
        elif stage == 'test':    
            if self.test_dataset is not None and not self.test_dataset.preloaded:
                self.logger.info("Preloading test dataset...")
                self.test_dataset.preload()
                        
        self.logger.info("Data preloading completed")
            
    def on_fit_start(self) -> None:
        """Called at the very beginning of fit to verify datasets are preloaded."""
        self.logger.info("Fit started - verifying datasets are preloaded")
        
        if isinstance(self.train_dataset, OnlineWindowDataset) or \
            isinstance(self.val_dataset, OnlineWindowDataset):
            self.logger.info("Dataset is OnlineWindowDataset - data is loaded on-the-fly, skipping preload verification")
            return
        
        if self.train_dataset is not None and not self.train_dataset.preloaded:
            self.logger.warning("Training dataset not preloaded - preloading now")
            self.train_dataset.preload()
            
        if self.val_dataset is not None and not self.val_dataset.preloaded:
            self.logger.warning("Validation dataset not preloaded - preloading now")
            self.val_dataset.preload()
        
    def on_validation_start(self) -> None:
        """Called at the beginning of validation to verify datasets are preloaded."""
        self.logger.info("Validation started - verifying datasets are preloaded")

        if isinstance(self.val_dataset, OnlineWindowDataset):
            self.logger.info("Dataset is OnlineWindowDataset - data is loaded on-the-fly, skipping preload verification")
            return

        if self.val_dataset is not None and not self.val_dataset.preloaded:
            self.logger.warning("Validation dataset not preloaded - preloading now")
            self.val_dataset.preload()
            
    def on_test_start(self) -> None:
        """Called at the beginning of testing to verify datasets are preloaded."""
        self.logger.info("Testing started - verifying datasets are preloaded")
        if isinstance(self.test_dataset, OnlineWindowDataset):
            self.logger.info("Dataset is OnlineWindowDataset - data is loaded on-the-fly, skipping preload verification")
            return
        if self.test_dataset is not None and not self.test_dataset.preloaded:
            self.test_dataset.preload()

    def train_dataloader(self) -> Union[torch.utils.data.DataLoader, None]:
        """Create the train dataloader.

        Returns:
            Train dataloader or None if dataset not available.
        """
        if self.train_dataset is None:
            return None
            
        train_config = self.dataloader_config["train"].copy()
        train_config["worker_init_fn"] = self.seed_worker
        
        return torch.utils.data.DataLoader(
            self.train_dataset,
            **train_config,
            collate_fn=padded_collate_fn,
        )

    def val_dataloader(self) -> Union[torch.utils.data.DataLoader, None]:
        """Create the validation dataloader.

        Returns:
            Validation dataloader or None if dataset not available.
        """
        if self.val_dataset is None:
            return None
            
        val_config = self.dataloader_config["val"].copy()
        val_config["worker_init_fn"] = self.seed_worker
        
        return torch.utils.data.DataLoader(
            self.val_dataset,
            **val_config,
            collate_fn=padded_collate_fn,
        )

    def test_dataloader(self) -> Union[torch.utils.data.DataLoader, None]:
        """Create the test dataloader.

        Returns:
            Test dataloader or None if dataset not available.
        """
        if self.test_dataset is None:
            return None
            
        test_config = self.dataloader_config["test"].copy()
        test_config["worker_init_fn"] = self.seed_worker
        
        return torch.utils.data.DataLoader(
            self.test_dataset,
            **test_config,
            collate_fn=padded_collate_fn,
        )
        
    def get_input_shape(self) -> Tuple[int, int, int]:
        """Get the input shape for model initialization.

        Returns:
            Tuple of (n_windows, n_channels, n_samples_per_window).
        """
        if self.input_shape is None:
            self.estimate_input_shape()
        assert self.input_shape is not None, "Input shape not set. Ensure datasets are preloaded."
        return self.input_shape[0], self.input_shape[1], self.input_shape[2]

    def get_output_shape(self) -> Tuple[int]:
        """Get the output shape for model initialization.

        Returns:
            Tuple of (n_windows,).
        """
        if self.output_shape is None:
            self.estimate_output_shape()
        assert self.output_shape is not None, "Output shape not set. Ensure datasets are preloaded."
        return self.output_shape[0],
    
    def estimate_input_shape(self) -> torch.Size:
        """Estimate input shape based on dataset configuration.

        Returns:
            Estimated input shape.
        """
        if self.input_shape is not None:
            return self.input_shape

        n_windows = self.dataset_config['n_windows']
        n_channels = len(self.good_channels)
        samples_per_window = int(self.dataset_config['sampling_rate'] * self.dataset_config['window_duration_s'])
        
        self.input_shape = torch.Size([n_windows, n_channels, samples_per_window])
        return self.input_shape

    def estimate_output_shape(self) -> torch.Size:
        """Estimate output shape based on dataset configuration.

        Returns:
            Estimated output shape.
        """
        if self.output_shape is not None:
            return self.output_shape

        n_windows = self.dataset_config['n_windows']
        self.output_shape = torch.Size([n_windows])
        return self.output_shape

    def get_class_weights(self) -> Dict[int, float]:
        """Get class weights for handling imbalance.

        Returns:
            Dictionary mapping class indices to weights.
        """
        self.logger.warning("Class weights not pre-calculated for on-the-fly processing. Returned balanced weights.")
        return {0: 1.0, 1: 1.0}

    def set_fold(self, fold: int) -> None:
        """Set the current fold for cross-validation.

        Args:
            fold: Fold number (1-based).
        """
        self.fold = fold
        self.logger.info(f"Set fold to {fold}")

        self.train_dataset = None
        self.val_dataset = None


def default_collate_fn(batch):
    """Custom collate function that handles metadata dictionaries.

    Args:
        batch: List of (data, labels) or (data, labels, metadata) tuples.

    Returns:
        Tuple of (batch_data, batch_labels, metadata).
    """
    data = [item[0] for item in batch]
    labels = [item[1] for item in batch]

    metadata = {}
    if len(batch[0]) > 2:
        for key in batch[0][2].keys():
            metadata[key] = [item[2][key] for item in batch]

    data = default_collate(data)
    labels = default_collate(labels)
    return data, labels, metadata


def padded_collate_fn(batch):
    """Pads variable-length window sequences in a batch.

    Args:
        batch: List of (data, label) or (data, label, metadata) tuples.

    Returns:
        Tuple of (batch_data, batch_label, window_mask, channel_mask) or with metadata list.
        Channel masks are extracted from metadata and stacked for per-sample masking.
    """
    has_meta = len(batch[0]) == 3
    meta_list = None
    if has_meta:
        data_list, label_list, meta_list = zip(*batch)
    else:
        data_list, label_list = zip(*batch)

    window_counts = [d.shape[0] for d in data_list]
    max_windows = max(window_counts)

    padded_data, padded_labels, window_mask_list = [], [], []
    channel_mask_list = []

    for i, (data, label) in enumerate(zip(data_list, label_list)):
        n = data.shape[0]
        pad = max_windows - n
        padded_data.append(torch.cat([data, torch.zeros(pad, *data.shape[1:])]))
        padded_labels.append(torch.cat([label, torch.zeros(pad)]))
        window_mask_list.append(torch.cat([torch.ones(n), torch.zeros(pad)]))

        if has_meta and meta_list:
            ch_mask = meta_list[i].get('channel_mask', None)
            if ch_mask is not None:
                if isinstance(ch_mask, list):
                    ch_mask = torch.tensor(ch_mask, dtype=torch.bool)
                elif not isinstance(ch_mask, torch.Tensor):
                    ch_mask = torch.tensor(ch_mask, dtype=torch.bool)
                channel_mask_list.append(ch_mask)
            else:
                n_channels = data.shape[1]
                channel_mask_list.append(torch.ones(n_channels, dtype=torch.bool))
        else:
            n_channels = data.shape[1]
            channel_mask_list.append(torch.ones(n_channels, dtype=torch.bool))

    batch_data = torch.stack(padded_data, dim=0)
    batch_label = torch.stack(padded_labels, dim=0)
    batch_window_mask = torch.stack(window_mask_list, dim=0)
    batch_channel_mask = torch.stack(channel_mask_list, dim=0) if channel_mask_list else None

    if has_meta and meta_list is not None:
        return batch_data, batch_label, batch_window_mask, batch_channel_mask, list(meta_list)
    return batch_data, batch_label, batch_window_mask, batch_channel_mask


class PredictionDataModule(L.LightningDataModule):
    """Lightning DataModule for prediction on single MEG files.

    Channel selection is handled automatically at inference time based on available
    channels in the MEG file.
    """

    def __init__(
        self,
        file_path: str,
        dataset_config: Dict[str, Any],
        dataloader_config: Dict[str, Any],
        **kwargs
    ):
        """Initialize prediction data module.

        Args:
            file_path: Path to the MEG file (.fif or .ds).
            dataset_config: Configuration for data processing.
            dataloader_config: Configuration for data loaders.
            **kwargs: Additional parameters for compatibility.
        """
        super().__init__()
        self.file_path = file_path
        self.dataset_config = dataset_config
        self.dataloader_config = dataloader_config

        self.predict_dataset: Optional[PredictDataset] = None
        self.input_shape: Optional[torch.Size] = None
        self.output_shape: Optional[torch.Size] = None
        
    def prepare_data(self):
        """Prepare data by verifying file exists.

        Raises:
            FileNotFoundError: If MEG file doesn't exist.
        """
        import os
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"MEG file not found: {self.file_path}")

        if not (self.file_path.endswith('.fif') or self.file_path.endswith('.ds') or
                self.file_path.endswith('.meg')):
            logger.warning(f"File type might not be supported: {self.file_path}")

    def setup(self, stage: Optional[str] = None):
        """Set up the prediction dataset.

        Args:
            stage: Current stage ('predict' or None).
        """
        if stage == 'predict' or stage is None:
            self.predict_dataset = PredictDataset(
                file_path=self.file_path,
                dataset_config=self.dataset_config,
            )

            if len(self.predict_dataset) > 0:
                sample = self.predict_dataset[0]
                data = sample[0]
                self.input_shape = data.shape
                self.output_shape = torch.Size([data.shape[0]])

                logger.info(f"Prediction dataset setup: {len(self.predict_dataset)} samples")
                logger.info(f"Input shape: {self.input_shape}")

    def predict_dataloader(self) -> torch.utils.data.DataLoader:
        """Create the prediction dataloader.

        Returns:
            Prediction dataloader.

        Raises:
            RuntimeError: If setup() hasn't been called.
        """
        if self.predict_dataset is None:
            raise RuntimeError("Call setup() before getting prediction dataloader")
            
        predict_config = self.dataloader_config.get('predict', self.dataloader_config.get('test', {}))
        if predict_config.get('shuffle', True):
            logger.warning("Shuffle should be False for prediction dataloader, setting to False")
            predict_config['shuffle'] = False

        def predict_collate_fn(batch):
            data_list = [item[0] for item in batch]
            metadata_list = [item[1] for item in batch]

            seg_counts = [d.shape[0] for d in data_list]
            max_segs = max(seg_counts)

            padded_data, window_mask_list = [], []
            channel_mask_list = []

            for i, data in enumerate(data_list):
                n = data.shape[0]
                pad = max_segs - n
                padded_data.append(torch.cat([data, torch.zeros(pad, *data.shape[1:])]))
                window_mask_list.append(torch.cat([torch.ones(n), torch.zeros(pad)]))

                if metadata_list and i < len(metadata_list):
                    ch_mask = metadata_list[i].get('channel_mask', None)
                    if ch_mask is not None:
                        if isinstance(ch_mask, list):
                            ch_mask = torch.tensor(ch_mask, dtype=torch.bool)
                        elif not isinstance(ch_mask, torch.Tensor):
                            ch_mask = torch.tensor(ch_mask, dtype=torch.bool)
                        channel_mask_list.append(ch_mask)
                    else:
                        n_channels = data.shape[1] if len(data.shape) > 1 else 1
                        channel_mask_list.append(torch.ones(n_channels, dtype=torch.bool))
                else:
                    n_channels = data.shape[1] if len(data.shape) > 1 else 1
                    channel_mask_list.append(torch.ones(n_channels, dtype=torch.bool))

            batch_data = torch.stack(padded_data, dim=0)
            batch_window_mask = torch.stack(window_mask_list, dim=0)
            batch_channel_mask = torch.stack(channel_mask_list, dim=0) if channel_mask_list else None

            return batch_data, batch_window_mask, batch_channel_mask, metadata_list

        return torch.utils.data.DataLoader(
            self.predict_dataset,
            **predict_config,
            collate_fn=predict_collate_fn,
        )

    def get_input_shape(self) -> torch.Size:
        """Get the input shape for model initialization.

        Returns:
            Input shape tensor.

        Raises:
            RuntimeError: If setup() hasn't been called.
        """
        if self.input_shape is None:
            raise RuntimeError("Call setup() before getting input shape")
        return self.input_shape
    
    def get_output_shape(self) -> torch.Size:
        """Get the output shape for model initialization.

        Returns:
            Output shape tensor.

        Raises:
            RuntimeError: If setup() hasn't been called.
        """
        if self.output_shape is None:
            raise RuntimeError("Call setup() before getting output shape")
        return self.output_shape
